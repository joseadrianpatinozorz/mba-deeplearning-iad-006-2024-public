{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui está um código simples com sklearn para testar a execução de Python no Codespace com uma Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.tree import plot_tree\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_dataset(dataset_func):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Essa função é usada para carregar um dataset do Scikit-learn, separando-o em dados e rótulos.\n",
    "\n",
    "    Parâmetros:\n",
    "        dataset_func: função\n",
    "            Função para carregar o dataset (por exemplo, `load_iris`, `load_digits`).\n",
    "\n",
    "    Retorno:\n",
    "        dataset: objeto\n",
    "            O dataset completo carregado.\n",
    "        X: array\n",
    "            As características (dados) do dataset.\n",
    "        y: array\n",
    "            Os rótulos do dataset.\n",
    "    \"\"\"\n",
    "    # Carregar o dataset especifico\n",
    "    dataset = dataset_func()\n",
    "    X = dataset.data\n",
    "    Y = dataset.target\n",
    "\n",
    "    return dataset, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dividir_treinamento(X, Y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\"\n",
    "    Descrição: \n",
    "        Função usada para Dividir matrizes ou matrizes em subconjuntos de trem aleatório e teste. para isso vou usar a função train_test_split()\n",
    "        \n",
    "    Parametrôs: data, label, test_size, random_state\n",
    "        X = Características de entrada\n",
    "        Y = Rotulos\n",
    "        test_size = representar a proporção do conjunto de dados para incluir na divisão de teste.\n",
    "        random_state = Controla o embaralhamento aplicado aos dados antes de aplicar a divisão. Passe um int para saída reprodutível em várias chamadas de função.\n",
    "\n",
    "    Retorno: X_train, X_test, y_train, y_test\n",
    "        X_train = dados de treinamento separado\n",
    "        X_test = dados de teste separado\n",
    "        y_train = rotulos de treinamento\n",
    "        y_test = rotulos de teste\n",
    "    \"\"\"\n",
    "    # Dividir o dataset em treinamento e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_dados(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Função para normalizar os dados de treinamento e teste usando StandardScaler.\n",
    "    \n",
    "    Parâmetros:\n",
    "        X_train: array-like\n",
    "            Dados de treinamento.\n",
    "        X_test: array-like\n",
    "            Dados de teste.\n",
    "    \n",
    "    Retorno:\n",
    "        X_train_normalizado: array-like\n",
    "            Dados de treinamento normalizados.\n",
    "        X_test_normalizado: array-like\n",
    "            Dados de teste normalizados.\n",
    "        scaler: StandardScaler\n",
    "            O objeto StandardScaler treinado, que pode ser usado para normalizar novos dados no futuro.\n",
    "    \"\"\"\n",
    "    # Instanciando o StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Ajustando o scaler e transformando os dados de treinamento\n",
    "    X_train_normalizado = scaler.fit_transform(X_train)\n",
    "    \n",
    "    # Transformando os dados de teste com o mesmo scaler\n",
    "    X_test_normalizado = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_normalizado, X_test_normalizado, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinando_modelo(modelo, X_train, y_train, X_test, random_state=None):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Função para treinar um modelo de classificação usando o sklearn.\n",
    "    \n",
    "    Parâmetros:\n",
    "        modelo: classificador\n",
    "            O modelo que queremos usar como classificador (por exemplo, DecisionTreeClassifier, RandomForestClassifier).\n",
    "        X_train: array-like\n",
    "            Dados de treinamento.\n",
    "        X_test: array-like\n",
    "            Dados de teste.\n",
    "        y_train: array-like\n",
    "            Rótulos do conjunto de treinamento.\n",
    "        random_state: int, opcional\n",
    "            Controla o embaralhamento aplicado aos dados antes de aplicar a divisão. Passe um int para saída reprodutível.\n",
    "\n",
    "    Retorno:\n",
    "        modelo_treinado: classificador\n",
    "            O modelo treinado.\n",
    "        y_pred: array\n",
    "            Previsões do modelo para o conjunto de teste.\n",
    "    \"\"\"\n",
    "    # Verificar se o modelo aceita o parâmetro random_state\n",
    "    if 'random_state' in modelo().get_params().keys():\n",
    "        modelo_treinado = modelo(random_state=random_state)\n",
    "    else:\n",
    "        modelo_treinado = modelo()\n",
    "\n",
    "    # Treinar o modelo\n",
    "    modelo_treinado.fit(X_train, y_train)\n",
    "\n",
    "    # Prever as classes para o conjunto de teste\n",
    "    y_pred = modelo_treinado.predict(X_test)\n",
    "\n",
    "    return modelo_treinado, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinando_com_grid_search(modelo, param_grid, X_train, y_train, X_test, cv=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Função para treinar um modelo de classificação usando GridSearchCV para encontrar os melhores hiperparâmetros.\n",
    "    \n",
    "    Parâmetros:\n",
    "        modelo: classificador\n",
    "            O modelo que queremos usar como classificador (por exemplo, DecisionTreeClassifier, RandomForestClassifier).\n",
    "        param_grid: dict\n",
    "            Dicionário contendo os hiperparâmetros que serão testados no GridSearchCV.\n",
    "        X_train: array-like\n",
    "            Dados de treinamento.\n",
    "        y_train: array-like\n",
    "            Rótulos do conjunto de treinamento.\n",
    "        X_test: array-like\n",
    "            Dados de teste.\n",
    "        cv: int, opcional (default=5)\n",
    "            Número de divisões para a validação cruzada.\n",
    "        random_state: int, opcional (default=42)\n",
    "            Controla o embaralhamento aplicado aos dados para garantir reprodutibilidade.\n",
    "\n",
    "    Retorno:\n",
    "        melhor_modelo: classificador\n",
    "            O melhor modelo treinado com os hiperparâmetros otimizados.\n",
    "        y_pred: array\n",
    "            Previsões do modelo para o conjunto de teste.\n",
    "        melhores_parametros: dict\n",
    "            Os melhores hiperparâmetros encontrados pelo GridSearchCV.\n",
    "    \"\"\"\n",
    "    # Instanciando o GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=modelo(random_state=random_state), param_grid=param_grid, cv=cv, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    # Executando o GridSearchCV para encontrar os melhores hiperparâmetros\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obter os melhores parâmetros\n",
    "    melhores_parametros = grid_search.best_params_\n",
    "    \n",
    "    # Treinar o modelo final com os melhores parâmetros\n",
    "    melhor_modelo = grid_search.best_estimator_\n",
    "    \n",
    "    # Prever as classes para o conjunto de teste\n",
    "    y_pred = melhor_modelo.predict(X_test)\n",
    "    \n",
    "    return melhor_modelo, y_pred, melhores_parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinando_com_grid_search(modelo, param_grid, X_train, y_train, X_test, cv=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Função para treinar um modelo de classificação usando GridSearchCV para encontrar os melhores hiperparâmetros.\n",
    "    \n",
    "    Parâmetros:\n",
    "        modelo: classificador\n",
    "            O modelo que queremos usar como classificador (por exemplo, DecisionTreeClassifier, RandomForestClassifier).\n",
    "        param_grid: dict\n",
    "            Dicionário contendo os hiperparâmetros que serão testados no GridSearchCV.\n",
    "        X_train: array-like\n",
    "            Dados de treinamento.\n",
    "        y_train: array-like\n",
    "            Rótulos do conjunto de treinamento.\n",
    "        X_test: array-like\n",
    "            Dados de teste.\n",
    "        cv: int, opcional (default=5)\n",
    "            Número de divisões para a validação cruzada.\n",
    "        random_state: int, opcional (default=42)\n",
    "            Controla o embaralhamento aplicado aos dados para garantir reprodutibilidade.\n",
    "\n",
    "    Retorno:\n",
    "        melhor_modelo: classificador\n",
    "            O melhor modelo treinado com os hiperparâmetros otimizados.\n",
    "        y_pred: array\n",
    "            Previsões do modelo para o conjunto de teste.\n",
    "        melhores_parametros: dict\n",
    "            Os melhores hiperparâmetros encontrados pelo GridSearchCV.\n",
    "    \"\"\"\n",
    "    # Instanciando o GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=modelo(random_state=random_state), param_grid=param_grid, cv=cv, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    # Executando o GridSearchCV para encontrar os melhores hiperparâmetros\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obter os melhores parâmetros\n",
    "    melhores_parametros = grid_search.best_params_\n",
    "    \n",
    "    # Treinar o modelo final com os melhores parâmetros\n",
    "    melhor_modelo = grid_search.best_estimator_\n",
    "    \n",
    "    # Prever as classes para o conjunto de teste\n",
    "    y_pred = melhor_modelo.predict(X_test)\n",
    "    \n",
    "    return melhor_modelo, y_pred, melhores_parametros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configurar_e_treinar_modelo_arvore_decisao(X_train, X_test, y_train, max_depth=None, min_samples_split=2, min_samples_leaf=1, max_features=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Função para configurar e treinar um modelo de árvore de decisão usando o conjunto de dados de treinamento.\n",
    "\n",
    "    Parâmetros:\n",
    "        X_train: array-like\n",
    "            Dados de treinamento.\n",
    "        y_train: array-like\n",
    "            Rótulos do conjunto de treinamento.\n",
    "        max_depth: int ou None, opcional (default=None)\n",
    "            Profundidade máxima da árvore. Se None, os nós são expandidos até todas as folhas serem puras.\n",
    "        min_samples_split: int, opcional (default=2)\n",
    "            O número mínimo de amostras necessárias para dividir um nó.\n",
    "        min_samples_leaf: int, opcional (default=1)\n",
    "            O número mínimo de amostras necessárias para estar em um nó folha.\n",
    "        max_features: int, float, string ou None, opcional (default=None)\n",
    "            O número de características a considerar ao procurar a melhor divisão.\n",
    "        random_state: int, opcional (default=42)\n",
    "            Controla o embaralhamento aplicado aos dados para garantir reprodutibilidade.\n",
    "\n",
    "    Retorno:\n",
    "        modelo_treinado: classificador\n",
    "            O modelo treinado de árvore de decisão.\n",
    "    \"\"\"\n",
    "    # Configurando o modelo de árvore de decisão\n",
    "    modelo = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Treinando o modelo\n",
    "    modelo_treinado = modelo.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = modelo_treinado.predict(X_test)\n",
    "\n",
    "    return modelo_treinado, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treinar_modelo_ensemble(X_train, y_train, X_test, n_estimators=100, max_depth=None, random_state=42):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Função para treinar um modelo Ensemble, especificamente um RandomForestClassifier, usando o conjunto de dados de treinamento.\n",
    "    \n",
    "    Parâmetros:\n",
    "        X_train: array-like\n",
    "            Dados de treinamento.\n",
    "        y_train: array-like\n",
    "            Rótulos do conjunto de treinamento.\n",
    "        X_test: array-like\n",
    "            Dados de teste.\n",
    "        n_estimators: int, opcional (default=100)\n",
    "            O número de árvores na floresta.\n",
    "        max_depth: int ou None, opcional (default=None)\n",
    "            Profundidade máxima da árvore. Se None, as árvores crescem até todas as folhas serem puras ou até que todas as folhas contenham menos que min_samples_split amostras.\n",
    "        random_state: int, opcional (default=42)\n",
    "            Controla o embaralhamento aplicado às árvores e amostras para garantir reprodutibilidade.\n",
    "\n",
    "    Retorno:\n",
    "        modelo_treinado: classificador\n",
    "            O modelo Ensemble treinado.\n",
    "        y_pred: array\n",
    "            Previsões do modelo para o conjunto de teste.\n",
    "    \"\"\"\n",
    "    # Configurando o modelo Ensemble - RandomForestClassifier\n",
    "    modelo = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Treinando o modelo\n",
    "    modelo_treinado = modelo.fit(X_train, y_train)\n",
    "\n",
    "    # Fazendo previsões\n",
    "    y_pred = modelo_treinado.predict(X_test)\n",
    "\n",
    "    return modelo_treinado, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train, y_train, X_test, y_test, max_depth=10, n_estimators=100, learning_rate=0.1, random_state=42):\n",
    "    \"\"\"\n",
    "    Treina um modelo XGBoost.\n",
    "    \n",
    "    Argumentos:\n",
    "        X_train: array numpy com os dados de treinamento.\n",
    "        y_train: array numpy com as labels de treinamento.\n",
    "        X_test: array numpy com os dados de test.\n",
    "        y_test: array numpy com as labels de test.\n",
    "        max_depth: profundidade máxima das árvores.\n",
    "        n_estimators: número de árvores na floresta.\n",
    "        learning_rate: taxa de aprendizado.\n",
    "        \n",
    "    Retorna:\n",
    "        modelo: o classificador XGBoost treinado.\n",
    "    \"\"\"\n",
    "    model_xgboost = xgb.XGBClassifier(max_depth=max_depth, n_estimators=n_estimators, learning_rate=learning_rate, random_state=random_state)\n",
    "    model_xgboost.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_xgboost = model_xgboost.predict(X_test)\n",
    "    \n",
    "    return model_xgboost, y_pred_xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_metricas(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Função para calcular as métricas de precisão, recall e F1-Score.\n",
    "    \n",
    "    Parâmetros:\n",
    "        y_test: array-like\n",
    "            Rótulos reais do conjunto de teste.\n",
    "        y_pred: array-like\n",
    "            Rótulos preditos pelo modelo.\n",
    "    \n",
    "    Retorno:\n",
    "        precision: float\n",
    "            A precisão ponderada do modelo.\n",
    "        recall: float\n",
    "            O recall ponderado do modelo.\n",
    "        f1: float\n",
    "            O F1-Score ponderado do modelo.\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotar_comparacao_metricas_linhas(labels, decision_tree_scores, ensemble_scores, xgboost_scores):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Função para plotar a comparação das métricas entre o modelo de Árvore de Decisão e o modelo Ensemble usando um gráfico de linhas.\n",
    "    \n",
    "    Parâmetros:\n",
    "        labels: list\n",
    "            Lista de rótulos para as métricas ('Precision', 'Recall', 'F1-Score').\n",
    "        decision_tree_scores: list\n",
    "            Lista de métricas do modelo de Árvore de Decisão.\n",
    "        ensemble_scores: list\n",
    "            Lista de métricas do modelo Ensemble.\n",
    "    \"\"\"\n",
    "    x = np.arange(len(labels))  # Índices das métricas\n",
    "    \n",
    "    # Criando o gráfico de linhas\n",
    "    plt.plot(x, decision_tree_scores, marker='o', linestyle='-', label='Decision Tree')\n",
    "    plt.plot(x, ensemble_scores, marker='o', linestyle='-', label='Ensemble (Random Forest)')\n",
    "    plt.plot(x, xgboost_scores, marker='o', linestyle='-', label='xgboost')\n",
    "    \n",
    "    # Adicionando detalhes ao gráfico\n",
    "    plt.xticks(ticks=x, labels=labels)\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('Comparação das Métricas entre Decision Tree, Ensemble e xgboost')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Exibir o gráfico\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizar_arvore(modelo, feature_names=None, class_names=None):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Função para visualizar a árvore de decisão treinada.\n",
    "    \n",
    "    Parâmetros:\n",
    "        modelo: classificador\n",
    "            O modelo de árvore de decisão treinado.\n",
    "        feature_names: list, opcional\n",
    "            Lista com os nomes das características (features).\n",
    "        class_names: list, opcional\n",
    "            Lista com os nomes das classes.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plot_tree(modelo, feature_names=feature_names, class_names=class_names, filled=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gini(p):\n",
    "    \"\"\"\n",
    "    Calcula o índice de Gini.\n",
    "    \n",
    "    Argumento:\n",
    "        p: lista de proporções de classes no nó.\n",
    "        \n",
    "    Retorna:\n",
    "        gini: valor da impureza Gini.\n",
    "    \"\"\"\n",
    "    return 1 - sum([pi**2 for pi in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(p):\n",
    "    \"\"\"\n",
    "    Calcula a entropia.\n",
    "    \n",
    "    Argumento:\n",
    "        p: lista de proporções de classes no nó.\n",
    "        \n",
    "    Retorna:\n",
    "        entropy: valor da entropia.\n",
    "    \"\"\"\n",
    "    return -sum([pi * np.log2(pi) for pi in p if pi > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impureza_no_raiz(modelo, X, y):\n",
    "    \"\"\"\n",
    "    Descrição:\n",
    "        Calcula a impureza de Gini no nó raiz de um modelo de árvore de decisão.\n",
    "    \n",
    "    Parâmetros:\n",
    "        modelo: classificador\n",
    "            O modelo de árvore de decisão treinado.\n",
    "        X: array-like\n",
    "            Conjunto de dados usado para o treinamento.\n",
    "        y: array-like\n",
    "            Rótulos associados ao conjunto de dados.\n",
    "    \n",
    "    Retorno:\n",
    "        impureza_gini: float\n",
    "            A impureza de Gini no nó raiz.\n",
    "    \"\"\"\n",
    "    # Pegar as amostras que chegaram ao nó raiz\n",
    "    node_indicator = modelo.decision_path(X)\n",
    "    node_index = node_indicator.indices[node_indicator.indptr[0]:node_indicator.indptr[1]]\n",
    "    \n",
    "    # Amostras no nó raiz\n",
    "    classes_no_raiz = y[node_index]\n",
    "    \n",
    "    # Calcular a impureza de Gini\n",
    "    impureza_gini = calculate_gini(classes_no_raiz)\n",
    "    \n",
    "    return impureza_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercício 1 - Treinamento do modelo baseado em árvore de decisão\n",
    "\n",
    "Neste exercício, você irá treinar um modelo de árvore de decisão utilizando o conjunto de dados MNIST. O objetivo é entender como funciona o processo de treinamento de um modelo de árvore de decisão e como ajustar os parâmetros para melhorar o desempenho do modelo.\n",
    "\n",
    "Você irá:\n",
    "\n",
    "Carregar o conjunto de dados MNIST e pré-processar os dados, se necessário.\n",
    "Selecionar a biblioteca ou framework que você irá utilizar (por exemplo, Scikit-learn e/ou XGBoost).\n",
    "Configurar os parâmetros do modelo, como a profundidade da árvore, o número de características a considerar em cada nó, etc.\n",
    "Treinar o modelo utilizando o conjunto de dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características (X): [[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      "  15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "   0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "   0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      " [ 0.  0.  0. 12. 13.  5.  0.  0.  0.  0.  0. 11. 16.  9.  0.  0.  0.  0.\n",
      "   3. 15. 16.  6.  0.  0.  0.  7. 15. 16. 16.  2.  0.  0.  0.  0.  1. 16.\n",
      "  16.  3.  0.  0.  0.  0.  1. 16. 16.  6.  0.  0.  0.  0.  1. 16. 16.  6.\n",
      "   0.  0.  0.  0.  0. 11. 16. 10.  0.  0.]\n",
      " [ 0.  0.  0.  4. 15. 12.  0.  0.  0.  0.  3. 16. 15. 14.  0.  0.  0.  0.\n",
      "   8. 13.  8. 16.  0.  0.  0.  0.  1.  6. 15. 11.  0.  0.  0.  1.  8. 13.\n",
      "  15.  1.  0.  0.  0.  9. 16. 16.  5.  0.  0.  0.  0.  3. 13. 16. 16. 11.\n",
      "   5.  0.  0.  0.  0.  3. 11. 16.  9.  0.]\n",
      " [ 0.  0.  7. 15. 13.  1.  0.  0.  0.  8. 13.  6. 15.  4.  0.  0.  0.  2.\n",
      "   1. 13. 13.  0.  0.  0.  0.  0.  2. 15. 11.  1.  0.  0.  0.  0.  0.  1.\n",
      "  12. 12.  1.  0.  0.  0.  0.  0.  1. 10.  8.  0.  0.  0.  8.  4.  5. 14.\n",
      "   9.  0.  0.  0.  7. 13. 13.  9.  0.  0.]\n",
      " [ 0.  0.  0.  1. 11.  0.  0.  0.  0.  0.  0.  7.  8.  0.  0.  0.  0.  0.\n",
      "   1. 13.  6.  2.  2.  0.  0.  0.  7. 15.  0.  9.  8.  0.  0.  5. 16. 10.\n",
      "   0. 16.  6.  0.  0.  4. 15. 16. 13. 16.  1.  0.  0.  0.  0.  3. 15. 10.\n",
      "   0.  0.  0.  0.  0.  2. 16.  4.  0.  0.]]\n",
      "Rótulos (y): [0 1 2 3 4]\n",
      "dados normalizados de treinamento [[ 0.         -0.34169755 -0.46336049 ...  1.05270303  0.45952251\n",
      "  -0.19710003]\n",
      " [ 0.         -0.34169755  0.78471641 ... -0.64451929 -0.50623083\n",
      "  -0.19710003]\n",
      " [ 0.         -0.34169755 -1.08739895 ... -0.13535259 -0.50623083\n",
      "  -0.19710003]\n",
      " ...\n",
      " [ 0.         -0.34169755  0.78471641 ...  1.56186972 -0.02335416\n",
      "  -0.19710003]\n",
      " [ 0.         -0.34169755 -0.87938613 ... -1.15368598 -0.50623083\n",
      "  -0.19710003]\n",
      " [ 0.         -0.34169755 -0.87938613 ... -0.98396375 -0.50623083\n",
      "  -0.19710003]]\n",
      "dados normalizados de teste [[ 0.         -0.34169755 -1.08739895 ...  1.22242526  0.70096084\n",
      "  -0.19710003]\n",
      " [ 0.         -0.34169755  1.20074205 ... -0.98396375 -0.50623083\n",
      "  -0.19710003]\n",
      " [ 0.         -0.34169755  0.57670359 ...  0.20409187 -0.50623083\n",
      "  -0.19710003]\n",
      " ...\n",
      " [ 0.         -0.34169755  0.36869078 ...  0.54353633 -0.50623083\n",
      "  -0.19710003]\n",
      " [ 0.         -0.34169755  0.36869078 ...  1.56186972  0.45952251\n",
      "  -0.19710003]\n",
      " [ 0.         -0.34169755  1.8247805  ... -0.30507483 -0.50623083\n",
      "  -0.19710003]]\n"
     ]
    }
   ],
   "source": [
    "#Exercicio 1\n",
    "dataset, X, Y = carregar_dataset(load_digits)\n",
    "# Agora, você pode usar `X` e `y` em outras partes do seu código\n",
    "print(\"Características (X):\", X[:5])\n",
    "print(\"Rótulos (y):\", Y[:5])\n",
    "#dividindo o treinamento\n",
    "X_train, X_test, y_train, y_test = dividir_treinamento(X, Y)\n",
    "\n",
    "#normalizando os dados\n",
    "X_train_normalizado, X_test_normalizado, scaler = normalizar_dados(X_train, X_test)\n",
    "\n",
    "print(\"dados normalizados de treinamento\", X_train_normalizado)\n",
    "print(\"dados normalizados de teste\", X_test_normalizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=gini, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=3, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=5, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=10, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=1, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=2, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=2; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=5; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n",
      "[CV] END criterion=entropy, max_depth=None, min_samples_leaf=4, min_samples_split=10; total time=   0.0s\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../model/modelo_DecisionTree_gridseach.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Chamando a função treinando_com_grid_search\u001b[39;00m\n\u001b[1;32m     10\u001b[0m modelo_DecisionTree_gridseach, y_pred_DecisionTree_gridseach, melhores_parametros \u001b[38;5;241m=\u001b[39m treinando_com_grid_search(DecisionTreeClassifier, param_grid, X_train_normalizado, y_train, X_test_normalizado)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelo_DecisionTree_gridseach\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../model/modelo_DecisionTree_gridseach.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Exibindo os melhores parâmetros encontrados\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelhores parâmetros encontrados:\u001b[39m\u001b[38;5;124m\"\u001b[39m, melhores_parametros)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/joblib/numpy_pickle.py:552\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    550\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[0;32m--> 552\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    553\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../model/modelo_DecisionTree_gridseach.pkl'"
     ]
    }
   ],
   "source": [
    "# Definindo o espaço de hiperparâmetros para o GridSearchCV\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Chamando a função treinando_com_grid_search\n",
    "modelo_DecisionTree_gridseach, y_pred_DecisionTree_gridseach, melhores_parametros = treinando_com_grid_search(DecisionTreeClassifier, param_grid, X_train_normalizado, y_train, X_test_normalizado)\n",
    "joblib.dump(modelo_DecisionTree_gridseach, '../model/modelo_DecisionTree_gridseach.pkl')\n",
    "# Exibindo os melhores parâmetros encontrados\n",
    "print(\"Melhores parâmetros encontrados:\", melhores_parametros)\n",
    "\n",
    "# Exibindo o relatório de classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred_DecisionTree_gridseach))\n",
    "print(\"Precisão:\", modelo_DecisionTree_gridseach.score(X_test_normalizado, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
